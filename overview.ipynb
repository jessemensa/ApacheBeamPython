{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline: \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE OPTIONS => USE THIS TO CONFIGURE DIFFERENT ASPECTS OF YOUR PIPELINE \n",
    "# IE as the pipeline runner that will execute your pipeline and any runner-specific configuration required by the chosen runner.\n",
    "from apache_beam.options.pipeline_options import PipelineOptions \n",
    "beam_options = PipelineOptions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# CREATING CUSTOM OPTIONS \n",
    "# FIRST NEED TO UNDERSTAND WHAT PYTHON DECORATORS ARE \n",
    "# STATIC VS DYNAMIC METHODS \n",
    "\n",
    "# STATIC METHODS => CAN BE EXECUTED THROUGH THE CLASS INSTEAD OF WHEN THE CLASS HAS BEEN INSTANTIATED OR OBJECT CAN BE CREATED FROM THE CLASS \n",
    "\n",
    "# WITHOUT STATIC METHOD => no attributes so method run directly without the class being instantiated \n",
    "class Formula:\n",
    "    def pow(self, x, y):\n",
    "        return x ** y \n",
    "\n",
    "m = Formula()\n",
    "print(m.pow(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# WITH A STATIC METHOD \n",
    "class Formula: \n",
    "    @staticmethod \n",
    "    def pow(x, y):\n",
    "        return x ** y \n",
    "\n",
    "print(Formula.pow(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ANOTHER EXAMPLE \n",
    "class Greeting: \n",
    "    def display(self, message):\n",
    "        print(message) \n",
    "message = Greeting() \n",
    "print(message.display('HelloWorld')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Greeting: \n",
    "    @staticmethod\n",
    "    def display(message):\n",
    "        print(message) \n",
    "print(Greeting.display('Hello World')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# WORKING WITH CLASS METHODS \n",
    "# CLASS METHODS => STATIC METHODS THAT ACCESS THE INSTANCE ATTRIBUTES & ALSO CAN BE CALLED WITHOUT INSTANTIATING THE CLASS \n",
    "class Greeting: \n",
    "    message = \"Hello\"\n",
    "    @classmethod\n",
    "    def display(mes, finalMessage):\n",
    "        print(mes.message + finalMessage) \n",
    "\n",
    "print(Greeting.display(' World'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING CUSTOM OPTIONS IN ADDITION TO THE STANDARD PIPELINE OPTIONS \n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "    @classmethod \n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument('--input', required=True)\n",
    "        parser.add_argument('--output', required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT HAS DEFAULT DATAFLOW LINK AND HELP TEXT \n",
    "# this allows pipeline to accept input and output as command line arguments \n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "    @classmethod \n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "        '--input',\n",
    "        default= 'gs://dataflow-samples/repairs/repairsfeedback.txt',\n",
    "        help='file path for the input text to process')\n",
    "        parser.add_argument(\n",
    "            '--output', \n",
    "            required=True, \n",
    "            help='The path prefix for the output files'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION => DISRTIBUTED COLLECTION OF DATA \n",
    "# BEAM TRANSFORMS USE PCOLLECTIONS AS INPUTS AND OUTPUTS \n",
    "# IF YOU WANT TO WORK WITH DATA IN YOUR PIPELINE \n",
    "# IT MUST COME FROM A PCOLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING FROM AN EXTERNAL SOURCE \n",
    "# HAVE TO PROVIDE THE LINK \n",
    "import apache_beam as beam \n",
    "\n",
    "lines = pipeline | 'Read the file' >> beam.io.ReadFromText('gs://dataflow-samples/repairs/repairsfeedback.txt') \n",
    "\n",
    "# DO NOT RUN THIS FILE\n",
    "# JUST READ THE CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A PCOLLECTION FROM AN IN MEMORY DATA \n",
    "import apache_beam as beam \n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    lines = (\n",
    "        pipeline \n",
    "        | beam.Create([\n",
    "            'This is true', \n",
    "            'To be, or not to be: that is the question: ',\n",
    "          \"Whether 'tis nobler in the mind to suffer \",\n",
    "          'The slings and arrows of outrageous fortune, ',\n",
    "          'Or to take arms against a sea of troubles, ',\n",
    "        ]))\n",
    "\n",
    "# NOTE: DO NOT RUN THIS AS IT REQUIRES AN OUTPUT TO WORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION FEATURES \n",
    "# PCOLLECTION BE ALWAYS OWNED BY A SPECIFIC PIPELINE \n",
    "# MULTIPLE PIPELINES NO FIT SHARE PCOLLECTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT TYPE \n",
    "# PCOLLECTIONS FIT BE OF ANY TYPE \n",
    "# BUT IF WE DEY DO DISTRIBUTED PROCESSING, BEAM FOR ENCODE THE ELEMENT TYPE AS A BYTE STRING \n",
    "# SO SAY ELEMENTS FIT BE PASSED AROUND TO DISTRIBUTED WORKERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT SCHEMA => JSON, AVRO, DB RECORDS \n",
    "# SCHEMA DEY HELP MAKE WE GET MORE EXPRESSIVE AGGREGATES \n",
    "# WHAT BE AGGREGATES ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTIONS BE IMMUTABLE \n",
    "# YOU NOT FIT CHANGE, ADD OR REMOVE ELEMENTS IF YOU CREATE AM \n",
    "# BEAM TRANSFORM JUST DEY MODIFY EACH ELEMENT DN GENERATE NEW PIPELINE \n",
    "# NO DEY MEAN SAY E GO MODIFY THE ORIGINAL PCOLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION FIT BE ANY SIZE \n",
    "# FIT BE BOUNDED OR UNBOUNDED => EITHER THE SIZE WE KNOW OR WE DONT KNOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT TIME STAMPS \n",
    "# EACH ELEMENT FOR PCOLLECTION GET SOME ASSOCIATED TIME STAMP \n",
    "# THIS BE ASSIGNED BY THE SOURCE WEY CREATE THE PCOLLECTION \n",
    "# SAY IF PIPELINE DEY READ TWEETS, EACH ELEMENT FIT USE THE TIME THE PERSON TWEET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMS \n",
    "# WHAT WE WANT TURN THE DATA INTO \n",
    "# WE GO GO OVER THIS FOR THE EXPERIMENT SECTION \n",
    "# PIPELINE PROCESS DEY LOOK LIKE THIS \n",
    "# DATABASE TABLE -> READ TRANSFORM -> (PCOLLECTION) -> TRANSFORM -> (PCOLLECTION) -> WRITE TRANSFORM -> DATABASE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION IS IMMUTABLE BY DEFINITION BUT \n",
    "# U FIT APPLY DIFFERENT TRANSFORMS FOR THE SAME PCOLLECTION TO CREATE A BRANCING PIPELINE LIKE SO \n",
    "# DATABASE TABLE -> READ DATABASE OF NAMES -> PARDO(EXTRACT STRINGS STARTING WITH A)-> A NAMES \n",
    "# DATABASE TABLE -> READ DATABASE OF NAMES -> PARDO(EXTRACT STRINGS STARTING WITH B)-> B NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CORE BEAM TRANSFORMS \n",
    "# PARDO => PROCESS ELEMENTS IN PARALLEL \n",
    "# GROUP BY KEY => GROUP ELEMENTS BY KEY \n",
    "# CO GROUP BY KEY => GROUP ELEMENTS BY KEY AND JOIN THEM \n",
    "# COMBINE => AGGREGATE ELEMENTS\n",
    "# FLATTEN => EXPAND A COLLECTION OF COLLECTIONS INTO A SINGLE COLLECTION\n",
    "# PARTITION => DIVIDE A COLLECTION INTO A NUMBER OF PARTITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARDO \n",
    "# FILTERING DATASET \n",
    "# TYPE CONVERTING EACH ELEMENT IN THE DATASET \n",
    "# EXTRACTING PARTS OF EACH ELEMENT IN THE DATASET \n",
    "# PERFORMING COMPUTATIONS ON EACH ELEMENT IN THE DATASET \n",
    "\n",
    "words = ... # input pcollection of strings \n",
    "\n",
    "# do function to perform on each element in the input pcollection\n",
    "class ComputeLength(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [len(element)] \n",
    "\n",
    "# apply the do function to the input pcollection \n",
    "word_length = words | beam.ParDo(ComputeLength()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT BE DO FUNCTION \n",
    "# THEN DEY DEFINE THE PIPLELINE EXTRACT PROCESSING TASKS \n",
    "# THE CODE FOR FULFIL THESE TWO REQUIREMENTS \n",
    "# MAKE SURE WE DEY FULFUL THIS BEFORE WE USE THE DO FUNCTIONS \n",
    "# FUNCTION OBJECT FOR BE SERLIALISABLE \n",
    "# FUNCTION OBJECT FOR THE THREAD-COMPATIBLE AND FOR KNOW SAY BEAM SDKs NO BE THREAD SAFE \n",
    "# ALSO MAKE THE FUNCTIONS IDEMPOTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THE FUNCTIONS BE STRAIGHFOWARD \n",
    "# WE FIT PROVIDE LIGHT WEIGHT DO FUNCTIONS ONE LINERS \n",
    "word_length = words | beam.ParDo(lambda x: [len(x)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO FUNCTION LIFECYCLE \n",
    "# DO DEEP INTO SERLLIALISATION AND DESERIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY KEY AND UNBOUNDED PCOLLECTIONS \n",
    "\n",
    "\n",
    "# U FOR USE NON-GLOBAL KEY WINDOWING OR AGGREGATION TRIGGER TO PERFORM GROUPBYKEY OR COGROUPBYKEY \n",
    "# UNBOUNDED COLLECTIONS \n",
    "# WINDOWING OR TRIGGERS FOR ALLOW GROUPING TO OPERATE ON LOGICAL FINITE BUNDLES OF DATA WITHIN THE \n",
    "# UNBOUNDED DATA STREAMS \n",
    "\n",
    "# IF WE NO USE AM E GO THROW THIS ERROR =>  IllegalStateException error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COGROUPBY KEY => JOIN KEY VALUE PAIRS WEY BE THE SAME TYPE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--runner RUNNER] [--streaming]\n",
      "                             [--resource_hint RESOURCE_HINTS]\n",
      "                             [--beam_services BEAM_SERVICES]\n",
      "                             [--type_check_strictness {ALL_REQUIRED,DEFAULT_TO_ANY}]\n",
      "                             [--type_check_additional TYPE_CHECK_ADDITIONAL]\n",
      "                             [--no_pipeline_type_check] [--runtime_type_check]\n",
      "                             [--performance_runtime_type_check]\n",
      "                             [--allow_non_deterministic_key_coders]\n",
      "                             [--allow_unsafe_triggers]\n",
      "                             [--no_direct_runner_use_stacked_bundle]\n",
      "                             [--direct_runner_bundle_repeat DIRECT_RUNNER_BUNDLE_REPEAT]\n",
      "                             [--direct_num_workers DIRECT_NUM_WORKERS]\n",
      "                             [--direct_running_mode {in_memory,multi_threading,multi_processing}]\n",
      "                             [--direct_embed_docker_python]\n",
      "                             [--dataflow_endpoint DATAFLOW_ENDPOINT]\n",
      "                             [--project PROJECT] [--job_name JOB_NAME]\n",
      "                             [--staging_location STAGING_LOCATION]\n",
      "                             [--temp_location TEMP_LOCATION] [--region REGION]\n",
      "                             [--service_account_email SERVICE_ACCOUNT_EMAIL]\n",
      "                             [--no_auth]\n",
      "                             [--template_location TEMPLATE_LOCATION]\n",
      "                             [--label LABELS] [--update]\n",
      "                             [--transform_name_mapping TRANSFORM_NAME_MAPPING]\n",
      "                             [--enable_streaming_engine]\n",
      "                             [--dataflow_kms_key DATAFLOW_KMS_KEY]\n",
      "                             [--create_from_snapshot CREATE_FROM_SNAPSHOT]\n",
      "                             [--flexrs_goal {COST_OPTIMIZED,SPEED_OPTIMIZED}]\n",
      "                             [--dataflow_service_option DATAFLOW_SERVICE_OPTIONS]\n",
      "                             [--enable_hot_key_logging]\n",
      "                             [--enable_artifact_caching]\n",
      "                             [--impersonate_service_account IMPERSONATE_SERVICE_ACCOUNT]\n",
      "                             [--gcp_oauth_scope GCP_OAUTH_SCOPES]\n",
      "                             [--hdfs_host HDFS_HOST] [--hdfs_port HDFS_PORT]\n",
      "                             [--hdfs_user HDFS_USER] [--hdfs_full_urls]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--max_num_workers MAX_NUM_WORKERS]\n",
      "                             [--autoscaling_algorithm {NONE,THROUGHPUT_BASED}]\n",
      "                             [--worker_machine_type MACHINE_TYPE]\n",
      "                             [--disk_size_gb DISK_SIZE_GB]\n",
      "                             [--worker_disk_type DISK_TYPE]\n",
      "                             [--worker_region WORKER_REGION]\n",
      "                             [--worker_zone WORKER_ZONE] [--zone ZONE]\n",
      "                             [--network NETWORK] [--subnetwork SUBNETWORK]\n",
      "                             [--worker_harness_container_image WORKER_HARNESS_CONTAINER_IMAGE]\n",
      "                             [--sdk_container_image SDK_CONTAINER_IMAGE]\n",
      "                             [--sdk_harness_container_image_overrides SDK_HARNESS_CONTAINER_IMAGE_OVERRIDES]\n",
      "                             [--default_sdk_harness_log_level DEFAULT_SDK_HARNESS_LOG_LEVEL]\n",
      "                             [--sdk_harness_log_level_overrides SDK_HARNESS_LOG_LEVEL_OVERRIDES]\n",
      "                             [--use_public_ips] [--no_use_public_ips]\n",
      "                             [--min_cpu_platform MIN_CPU_PLATFORM]\n",
      "                             [--dataflow_worker_jar DATAFLOW_WORKER_JAR]\n",
      "                             [--dataflow_job_file DATAFLOW_JOB_FILE]\n",
      "                             [--experiment EXPERIMENTS]\n",
      "                             [--number_of_worker_harness_threads NUMBER_OF_WORKER_HARNESS_THREADS]\n",
      "                             [--profile_cpu] [--profile_memory]\n",
      "                             [--profile_location PROFILE_LOCATION]\n",
      "                             [--profile_sample_rate PROFILE_SAMPLE_RATE]\n",
      "                             [--requirements_file REQUIREMENTS_FILE]\n",
      "                             [--requirements_cache REQUIREMENTS_CACHE]\n",
      "                             [--requirements_cache_only_sources]\n",
      "                             [--setup_file SETUP_FILE]\n",
      "                             [--beam_plugin BEAM_PLUGINS]\n",
      "                             [--pickle_library {cloudpickle,default,dill}]\n",
      "                             [--save_main_session]\n",
      "                             [--sdk_location SDK_LOCATION]\n",
      "                             [--extra_package EXTRA_PACKAGES]\n",
      "                             [--prebuild_sdk_container_engine PREBUILD_SDK_CONTAINER_ENGINE]\n",
      "                             [--prebuild_sdk_container_base_image PREBUILD_SDK_CONTAINER_BASE_IMAGE]\n",
      "                             [--cloud_build_machine_type CLOUD_BUILD_MACHINE_TYPE]\n",
      "                             [--docker_registry_push_url DOCKER_REGISTRY_PUSH_URL]\n",
      "                             [--job_endpoint JOB_ENDPOINT]\n",
      "                             [--artifact_endpoint ARTIFACT_ENDPOINT]\n",
      "                             [--job_server_timeout JOB_SERVER_TIMEOUT]\n",
      "                             [--environment_type ENVIRONMENT_TYPE]\n",
      "                             [--environment_config ENVIRONMENT_CONFIG]\n",
      "                             [--environment_option ENVIRONMENT_OPTIONS]\n",
      "                             [--sdk_worker_parallelism SDK_WORKER_PARALLELISM]\n",
      "                             [--environment_cache_millis ENVIRONMENT_CACHE_MILLIS]\n",
      "                             [--output_executable_path OUTPUT_EXECUTABLE_PATH]\n",
      "                             [--artifacts_dir ARTIFACTS_DIR]\n",
      "                             [--job_port JOB_PORT]\n",
      "                             [--artifact_port ARTIFACT_PORT]\n",
      "                             [--expansion_port EXPANSION_PORT]\n",
      "                             [--job_server_java_launcher JOB_SERVER_JAVA_LAUNCHER]\n",
      "                             [--job_server_jvm_properties JOB_SERVER_JVM_PROPERTIES]\n",
      "                             [--flink_master FLINK_MASTER]\n",
      "                             [--flink_version {1.12,1.13,1.14,1.15}]\n",
      "                             [--flink_job_server_jar FLINK_JOB_SERVER_JAR]\n",
      "                             [--flink_submit_uber_jar]\n",
      "                             [--parallelism PARALLELISM]\n",
      "                             [--max_parallelism MAX_PARALLELISM]\n",
      "                             [--spark_master_url SPARK_MASTER_URL]\n",
      "                             [--spark_job_server_jar SPARK_JOB_SERVER_JAR]\n",
      "                             [--spark_submit_uber_jar]\n",
      "                             [--spark_rest_url SPARK_REST_URL]\n",
      "                             [--spark_version {2,3}]\n",
      "                             [--on_success_matcher ON_SUCCESS_MATCHER]\n",
      "                             [--dry_run DRY_RUN]\n",
      "                             [--wait_until_finish_duration WAIT_UNTIL_FINISH_DURATION]\n",
      "                             [--pubsub_root_url PUBSUBROOTURL]\n",
      "                             [--s3_access_key_id S3_ACCESS_KEY_ID]\n",
      "                             [--s3_secret_access_key S3_SECRET_ACCESS_KEY]\n",
      "                             [--s3_session_token S3_SESSION_TOKEN]\n",
      "                             [--s3_endpoint_url S3_ENDPOINT_URL]\n",
      "                             [--s3_region_name S3_REGION_NAME]\n",
      "                             [--s3_api_version S3_API_VERSION]\n",
      "                             [--s3_verify S3_VERIFY] [--s3_disable_ssl]\n",
      "                             [--input INPUT] --output OUTPUT\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessmensa/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:3386: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# COMBINE => COMBINING ELEMENTS OR VALUES IN PCOLLECTIOONS \n",
    "pc = [1, 10, 100, 1000] \n",
    "\n",
    "def boundedsum(values, bounds=500):\n",
    "    return min(sum(values), bounds) \n",
    "\n",
    "small_sum = pc | beam.CombineGlobally(boundedsum)\n",
    "large_sum = pc | beam.CombineGlobally(boundedsum, bounds=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED COMBINATIONS USING COMBINEFN \n",
    "# NB: NEED TO UNDERSTAND COMBINERS WELL \n",
    "# NOTE: ALL COMBINERS SHOULD HAVE A MORE SOPHISTICATED ACCUMULATOR \n",
    "\n",
    "\n",
    "# FOR MORE COMPLEX FUNCTIONS, DEFINE A SUBCLASS OF COMBINEFN \n",
    "# YOU SHOULD USE A COMBINE FN IF FUNCTION REQUIRES A MORE SOPHISTICATED ACCUMULATOR \n",
    "\n",
    "# GENERAL COMBINING OPERATION CONSISTS OF FOUR STEPS \n",
    "# 1. CREATE AN ACCUMULATOR 2. ADD INPUT 3. MERGE ACCUMULATORS 4. EXTRACT OUTPUT \n",
    "\n",
    "pc = ... \n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0) \n",
    "\n",
    "    def add_input(self, sum_count, input):\n",
    "        (sum, count) = sum_count \n",
    "        return sum + input, count + 1 \n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators) \n",
    "        return sum(sums), sum(counts) \n",
    "\n",
    "    def extract_output(self, sum_count):\n",
    "        (sum, count) = sum_count \n",
    "        return sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINING ALL PCOLLCTIONS INTO A SINGLE VALUE \n",
    "\n",
    "pc = ... \n",
    "\n",
    "average = pc | beam.CombineGlobally(AverageFn()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE AND GLOBAL WINDOWING ?? COME BACK TO THIS PART \n",
    "# what is global windowing in the first place ?? \n",
    "# => If your input PCollection uses the default global windowing, the default behavior is to return a\n",
    "#  PCollection containing one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE VALUES in a keyed PCOLLECTION ?? COME BACK TO THIS PART TOO  \n",
    "player_accuracies = ...\n",
    "\n",
    "averageaccuracyperplayer = (\n",
    "    player_accuracies \n",
    "    | beam.CombinePerKey(beam.combiners.MeanCombineFn())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTEN => MERGE MULTIPLE PCOLLECTIONS INTO A SINGLE PCOLLECTION \n",
    "merged = (\n",
    "    (pcoll1, pcoll2, pcoll3) \n",
    "    | beam.Flatten() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ENCODING IN MERGED COLLECTIONS \n",
    "#  the coder for the output PCollection is the same as the coder for the first PCollection \n",
    "# in the input PCollectionList.\n",
    "\n",
    "# MERGING WINDOWED COLLECTIONS\n",
    "# When using Flatten to merge PCollection objects that have a windowing strategy applied, all of the \n",
    "# PCollection objects you want to merge must use a compatible windowing strategy and window sizing.\n",
    "#  For example, all the collections you’re merging must all use (hypothetically) identical 5-minute \n",
    "# fixed windows or 4-minute sliding windows starting every 30 seconds.\n",
    "# If your pipeline attempts to use Flatten to merge PCollection objects with incompatible windows, \n",
    "# Beam generates an IllegalStateException error when your pipeline is constructed.\n",
    "\n",
    "# PARTITION \n",
    "# Partition is a Beam transform for PCollection objects that store the same data type. \n",
    "# Partition splits a single PCollection into a fixed number of smaller collections.\n",
    "# Partition divides the elements of a PCollection according to a partitioning function that you provide.\n",
    "# The partitioning function contains the logic that determines how to split up the elements of the \n",
    "# input PCollection into each resulting partition PCollection.\n",
    "\n",
    "def partition_fn(student, num_partitions):\n",
    "    return int(get_percenitile(student) * num_partitions / 100) \n",
    "by_decile = students | beam.Partition(partition_fn, 10)\n",
    "fortieth_percentile = by_decile[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS FOR WRITING USER CODE FOR BEAM TRANSFORMS \n",
    "\n",
    "# Your function object must be able to be serialized, so it can be sent over the network to different machines.\n",
    "# Your function object must be thread-compatible, meaning it should be able to run \n",
    "# in parallel on multiple threads without any issues.\n",
    "# It is recommended that your function object be idempotent, meaning that it produces \n",
    "# the same result no matter how many times it is run. Non-idempotent functions can still be used with Beam, \n",
    "# but they require additional consideration to ensure correct behavior when there are external side effects.\n",
    "\n",
    "# These requirements apply to subclasses of DoFn (a function used with the ParDo transform), \n",
    "# CombineFn (a function used with the Combine transform), \n",
    "# and WindowFn (a function used with the Window transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERIALIZABILITY \n",
    "# In order to use a function object with a Beam transform, it must be fully serializable. \n",
    "# This means that it can be converted into a format that can be transmitted over \n",
    "# the network to a remote worker. \n",
    "\n",
    "# The base classes for user code, such as DoFn, CombineFn, and WindowFn, are already set up to be serializable, \n",
    "# but your subclass must not contain any non-serializable members.\n",
    "\n",
    "# FEW OTHER THINGS TO KEEP IN MIND \n",
    "# Transient fields in your function object will not be transmitted to worker instances, \n",
    "# because they are not automatically serialized.\n",
    "\n",
    "# You should avoid loading a large amount of data into a field before serialization, as this can impact performance.\n",
    "\n",
    "# Individual instances of your function object cannot share data.\n",
    "\n",
    "# Mutating a function object after it has been applied will not have any effect, \n",
    "# as the worker instances operate independently.\n",
    "\n",
    "# It is important to consider these factors when designing your function object, \n",
    "# as non-serializable functions will not be able to be transmitted to the worker instances and \n",
    "# will cause your pipeline to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THREAD COMPATIBILITY \n",
    "# Your function object should be thread-compatible. Each instance of your function object is accessed by a \n",
    "# single thread at a time on a worker instance, unless you explicitly create your own threads. Note, however, \n",
    "# that the Beam SDKs are not thread-safe. If you create your own threads in your user code, you must \n",
    "# provide your own synchronization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEMPOTENCY \n",
    "# It’s recommended that you make your function object idempotent–that is, that it can be repeated \n",
    "# or retried as often as necessary without causing unintended side effects.\n",
    "# Non-idempotent functions are supported, however the Beam model provides no guarantees as to the \n",
    "# number of times your user code might be invoked or retried;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIDE INPUTS \n",
    "# Side inputs are an additional source of data that can be accessed by a ParDo transform while\n",
    "# processing elements in the main input PCollection. \n",
    "# They are useful when your ParDo needs to use additional data that is determined at runtime and \n",
    "# not hard-coded, such as values that depend on the input data or another branch of the pipeline. \n",
    "# Side inputs allow you to provide this additional data to your ParDo transform in the form of \n",
    "# a view that can be read while processing each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSING SIDE INPUTS TO PARDO \n",
    "def filter_using_length(word, lower_bound, upper_bound=float('inf')):\n",
    "    if lower_bound <= len(word) <= upper_bound:\n",
    "        yield word \n",
    "\n",
    "# construct a deferred side input \n",
    "avg_word_len = (\n",
    "    words \n",
    "    | beam.Map(len) \n",
    "    | beam.CombinedGlobally(beam.combiners.MeanCombineFn()) \n",
    "\n",
    "    # call with explicit side input \n",
    "    small_words = words | 'small' >> beam.FlatMap(filter_using_length, 0, 3) \n",
    "\n",
    "    # A single deferred side input \n",
    "    larger_than_average = (\n",
    "        words | 'large' >> beam.FlatMap(filter_using_length, lower_bound = pvalue.AsSingleton(avg_word_len)) \n",
    "    )\n",
    "\n",
    "    # Mix and Match \n",
    "    small_but_nontrvial = words | beam.FlatMap(\n",
    "        filter_using_length, \n",
    "        lower_bound = 2, \n",
    "        upper_bound = pvalue.AsSingleton(avg_word_len) \n",
    "    )\n",
    "\n",
    "    # We can also pass side inputs to a Pardo transform, which will get passed to its process method \n",
    "    # The first two arguments for the process method will be self amd element \n",
    "    class FilterUsingLength(beam.DoFn):\n",
    "        def process(self, element, lower_bound, upper_bound=float('inf')):\n",
    "            if lower_bound <= len(element) <= upper_bound:\n",
    "                yield element \n",
    "    small_words = words | beam.ParDo(filterUsingLength(), 0, 3)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIDE INPUTS AND WINDOWING \n",
    "# A window is a way to divide up a stream of data into chunks, based on some criteria. For example, \n",
    "# you could create windows of data based on fixed time intervals (e.g. every 10 minutes), or based \n",
    "# on the data itself (e.g. all data belonging to the same user).\n",
    "\n",
    "# SIDE INPUT, is a way to provide additional data to a Beam pipeline that is needed for processing the main input data.\n",
    "#  This additional data is typically needed for a limited time, and is not part of the main input stream.\n",
    "\n",
    "# When you use a side input in a Beam pipeline, the main input and side input may have different windowing. \n",
    "# In this case, Beam projects the main input window onto the side input window set, and selects the most \n",
    "# appropriate side input value to use.\n",
    "# For example, if the main input is windowed by fixed-time intervals of one minute, and the side input is windowed by \n",
    "# fixed-time intervals of one hour, Beam will select the side input value from the appropriate hour-long window.\n",
    "\n",
    "# If the main input element exists in more than one window, then the processElement function \n",
    "# will be called multiple times, once for each window. In this case, each call to processElement \n",
    "# will project the current window for the main input element, and may provide a different \n",
    "# view of the side input each time.\n",
    "\n",
    "# If the side input has multiple trigger firings (i.e. it has been updated multiple times), \n",
    "# Beam will use the value from the latest trigger firing. This is useful if you are using \n",
    "# a side input with a single global window and a trigger, as it ensures that you are always\n",
    "#  using the most up-to-date side input value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIONAL OUTPUTS \n",
    "# While ParDo always produces a main output PCollection (as the return value from apply),\n",
    "# you can also have your ParDo produce any number of additional output PCollections. \n",
    "# If you choose to have multiple outputs, your ParDo returns all of the output PCollections (including the main output) \n",
    "# bundled together\n",
    "\n",
    "# To emit elements to multiple output PCollections, invoke with_outputs() on the ParDo, and specify the\n",
    "# expected tags for the outputs. with_outputs() returns a DoOutputsTuple object. Tags specified in\n",
    "# with_outputs are attributes on the returned DoOutputsTuple object. The tags give access to the\n",
    "# corresponding output PCollections.\n",
    "\n",
    "results = (\n",
    "    words \n",
    "    | beam.ParDo(ProcessWords(), cutoff_length=2, marker='X').with_outputs(\n",
    "        'above cutoff lenghts', \n",
    "        'marked strings', \n",
    "        main='below_cutoff_strings'\n",
    "    )\n",
    ")\n",
    "\n",
    "below = results.below_cutoff_strings \n",
    "above = results.above_cutoff_lengths \n",
    "marked = results['marked strings']\n",
    "\n",
    "below, above, marked = (\n",
    "    words \n",
    "    | beam.ParDo(ProcessWords(), cutoff_length=2, marker='X').with_outputs('above_cutoff_lengths',\n",
    "                                      'marked strings',\n",
    "                                      main='below_cutoff_strings')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMITTING MULTIPLE OUTPUTS IN YOUR DOFN \n",
    "# Inside your ParDo's DoFn, you can emit an element to a specific output by wrapping the value and the output tag (str).\n",
    "# using the pvalue.OutputValue wrapper class.\n",
    "# Based on the previous example, this shows the DoFn emitting to the main output and two additional outputs\n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, cutoff_length, marker): \n",
    "        if len(element) <= cutoff_length: \n",
    "            yield element \n",
    "        else: \n",
    "            yield pvalue.TaggedOutput('above_cutoff_lengths', len(element))\n",
    "        if element.startswith(marker):\n",
    "            yield pvalue.TaggedOutput('marked strings', element)\n",
    "\n",
    "# Producing multiple products is also available in Map and FlatMap \n",
    "# Here is an example that uses FlatMap and shows that the tags do not need to be specified ahead of time.\n",
    "def even_odd(x):\n",
    "    yield pvalue.TaggedOutput('odd' if x % 2 else 'even', x)\n",
    "    if x % 10 == 0:\n",
    "        yield x \n",
    "\n",
    "results = numbers | beam.FlatMap(even_odd).with_outputs() \n",
    "evens = result.even \n",
    "odds = result.odd \n",
    "tens = results[None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCESSING ADDITIONAL PARAMETERS IN YOUR DOFNs \n",
    "# In addition to the element, Beam will populate other parameters to your DoFn’s process method.\n",
    "# Timestamp: To access the timestamp of an input element, add a keyword parameter default to DoFn.TimestampParam\n",
    "class ProcessRecord(beam.FoFn):\n",
    "    def process(self, element, timestamp=beam.DoFn, TimestampParam):\n",
    "        # access timestamp of element \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window => To access the window an input element falls into, add a keyword parameter default to DoFn.WindowParam.\n",
    "# => If an element falls in multiple windows (for example, this will happen when using SlidingWindows), \n",
    "# then the process method will be invoked multiple time for the element, once for each window.\n",
    "class ProcessRecord(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.windowParam):\n",
    "        # access window \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaneInfo \n",
    "# => When triggers are used, Beam provides a DoFn.PaneInfoParam object that contains information about the current firing.\n",
    "# => Using DoFn.PaneInfoParam you can determine whether this is an early or a late firing, \n",
    "# and how many times this window has already fired for this key.\n",
    "\n",
    "class ProcessRecord(beam.DoFn):\n",
    "  def process(self, element, pane_info=beam.DoFn.PaneInfoParam):\n",
    "     # access pane info, e.g. pane_info.is_first, pane_info.is_last, pane_info.timing\n",
    "     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMER AND STATE \n",
    "# => In addition to aforementioned parameters, user defined Timer and State parameters can be used in a stateful DoFn.\n",
    "class StatefulDoFn(beam.DoFn):\n",
    "  \"\"\"An example stateful DoFn with state and timer\"\"\"\n",
    "\n",
    "  BUFFER_STATE_1 = BagStateSpec('buffer1', beam.BytesCoder())\n",
    "  BUFFER_STATE_2 = BagStateSpec('buffer2', beam.VarIntCoder())\n",
    "  WATERMARK_TIMER = TimerSpec('watermark_timer', TimeDomain.WATERMARK)\n",
    "\n",
    "  def process(self,\n",
    "              element,\n",
    "              timestamp=beam.DoFn.TimestampParam,\n",
    "              window=beam.DoFn.WindowParam,\n",
    "              buffer_1=beam.DoFn.StateParam(BUFFER_STATE_1),\n",
    "              buffer_2=beam.DoFn.StateParam(BUFFER_STATE_2),\n",
    "              watermark_timer=beam.DoFn.TimerParam(WATERMARK_TIMER)):\n",
    "\n",
    "    # Do your processing here\n",
    "    key, value = element\n",
    "    # Read all the data from buffer1\n",
    "    all_values_in_buffer_1 = [x for x in buffer_1.read()]\n",
    "\n",
    "    if StatefulDoFn._is_clear_buffer_1_required(all_values_in_buffer_1):\n",
    "        # clear the buffer data if required conditions are met.\n",
    "        buffer_1.clear()\n",
    "\n",
    "    # add the value to buffer 2\n",
    "    buffer_2.add(value)\n",
    "\n",
    "    if StatefulDoFn._all_condition_met():\n",
    "      # Clear the timer if certain condition met and you don't want to trigger\n",
    "      # the callback method.\n",
    "      watermark_timer.clear()\n",
    "\n",
    "    yield element\n",
    "\n",
    "  @on_timer(WATERMARK_TIMER)\n",
    "  def on_expiry_1(self,\n",
    "                  timestamp=beam.DoFn.TimestampParam,\n",
    "                  window=beam.DoFn.WindowParam,\n",
    "                  key=beam.DoFn.KeyParam,\n",
    "                  buffer_1=beam.DoFn.StateParam(BUFFER_STATE_1),\n",
    "                  buffer_2=beam.DoFn.StateParam(BUFFER_STATE_2)):\n",
    "    # Window and key parameters are really useful especially for debugging issues.\n",
    "    yield 'expired1'\n",
    "\n",
    "  @staticmethod\n",
    "  def _all_condition_met():\n",
    "      # some logic\n",
    "      return True\n",
    "\n",
    "  @staticmethod\n",
    "  def _is_clear_buffer_1_required(buffer_1_data):\n",
    "      # Some business logic\n",
    "      return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPOSITE TRANSFORMS \n",
    "# => Transforms can have a nested structure, where a complex transform performs multiple simpler transforms \n",
    "# (such as more than one ParDo, Combine, GroupByKey, or even other composite transforms).\n",
    "# =>  These transforms are called composite transforms. Nesting multiple transforms inside a single \n",
    "# composite transform can make your code more modular and easier to understand.\n",
    "# => The Beam SDK comes packed with many useful composite transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF COMPOSITE TRANSFORMS \n",
    "# The CountWords transform in the WordCount example program is an example of a composite transform. \n",
    "# CountWords is a PTransform subclass that consists of multiple nested transforms.\n",
    "\n",
    "# In its expand method, the CountWords transform applies the following transform operations:\n",
    "# 1. It applies a ParDo on the input PCollection of text lines, producing an output PCollection of individual words.\n",
    "# 2. It applies the Beam SDK library transform Count on the PCollection of words, producing a PCollection of key/value pairs.\n",
    "# Each key represents a word in the text, and each value represents the number of times that word appeared in the original data.\n",
    "# Note: Because Count is itself a composite transform, CountWords is also a nested composite transform.\n",
    "@beam.ptransform_fn \n",
    "def CountWords(pcoll):\n",
    "    return (\n",
    "        pcoll\n",
    "      # Convert lines of text into individual words.\n",
    "      | 'ExtractWords' >> beam.ParDo(ExtractWordsFn())\n",
    "      # Count the number of times each word occurs.\n",
    "      | beam.combiners.Count.PerElement()\n",
    "      # Format each word and count into a printable string.\n",
    "      | 'FormatCounts' >> beam.ParDo(FormatCountsFn()))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a composite transform \n",
    "# To create your own composite transform, create a subclass of the PTransform class and override the\n",
    "# expand method to specify the actual processing logic \n",
    "# You can then use this transform just as you would a built-in transform from the Beam SDK\n",
    "# The following code sample shows how to declare a PTransform that accepts a PCollection of Strings for input, \n",
    "# and outputs a PCollection of Integers:\n",
    "\n",
    "class ComputeWordLengths(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return pcoll | beam.Map(lambda x: len(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within your PTransform subclass, you’ll need to override the expand method. \n",
    "# The expand method is where you add the processing logic for the PTransform. \n",
    "# Your override of expand must accept the appropriate type of input PCollection \n",
    "# as a parameter, and specify the output PCollection as the return value.\n",
    "\n",
    "# The following code sample shows how to override expand for the ComputeWordLengths class declared \n",
    "class ComputeWordlengths(beam.Ptransform):\n",
    "    def expand(self, pcoll):\n",
    "        return pcoll | beam.Map(lambda x: len(x)) \n",
    "\n",
    "\n",
    "# As long as you override the expand method in your PTransform subclass to accept the \n",
    "# appropriate input PCollection(s) and return the corresponding output PCollection(s), \n",
    "# you can include as many transforms as you want\n",
    "# These transforms can include core transforms, composite transforms, or the transforms included in the Beam SDK libraries.\n",
    "\n",
    "# Your composite transform’s parameters and return value must match the initial input type and \n",
    "# final return type for the entire transform, even if the transform’s intermediate data changes type multiple times.\n",
    "\n",
    "# Note: The expand method of a PTransform is not meant to be invoked directly by the user of a transform. \n",
    "#  Instead, you should call the apply method on the PCollection itself, with the transform as an argument. \n",
    "#  This allows transforms to be nested within the structure of your pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTRANSFORM STYLE GUIDE \n",
    "# The PTransform Style Guide contains additional information not included here, such as \n",
    "# style guidelines, logging and testing guidance, \n",
    "# and language-specific considerations.\n",
    "# The guide is a useful starting point when you want to write new composite PTransforms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE I/O \n",
    "# When you create a pipeline, you often need to read data from some external source, \n",
    "# such as a file or a database. Likewise, you may want your pipeline to output \n",
    "# its result data to an external storage system. Beam provides read and write transforms \n",
    "# for a number of common data storage types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING INPUT FROM DATA \n",
    "lines = pipeline | beam.io.ReadFromText('gs://some/inputData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITING OUTPUT TO DATA \n",
    "output | beam.io.WriteToText('gs://some/outputData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK TO ALL I/O CONNECTORS \n",
    "https://beam.apache.org/documentation/io/connectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITING MULTIPLE OUTPUT FILES \n",
    "filtered_words | 'WriteToText' >> beam.io.WriteToText(\n",
    "    '/path/to/numbers', file_name_suffix='.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMAS \n",
    "# Often, the types of the records being processed have an obvious structure.\n",
    "#  Common Beam sources produce JSON, Avro, Protocol Buffer, or database row objects; \n",
    "# all of these types have well defined structures, structures\n",
    "#  that can often be determined by examining the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS SCHEMA \n",
    "# Purchase\n",
    "\n",
    "# Field Name\tField Type\n",
    "# userId\tSTRING\n",
    "# itemId\tINT64\n",
    "# shippingAddress\tROW(ShippingAddress)\n",
    "# cost\tINT64\n",
    "# transactions\tARRAY[ROW(Transaction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMAS FOR PYTHON \n",
    "import typing \n",
    "\n",
    "class Purchase(typing.NamedTuple):\n",
    "    userId: str\n",
    "    itemId: int\n",
    "    shippingAddress: 'ShippingAddress'\n",
    "    cost: int\n",
    "    transactions: typing.List['Transaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE DOCUMENTATION \n",
    "# BEGINNER UDEMY \n",
    "# APACHE BEAM DOCS => WORD COUNT, MOBILE GAMING, \n",
    "# APACHE BEAM IN JAVA TUTORIAL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
