{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline: \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE OPTIONS => USE THIS TO CONFIGURE DIFFERENT ASPECTS OF YOUR PIPELINE \n",
    "# IE as the pipeline runner that will execute your pipeline and any runner-specific configuration required by the chosen runner.\n",
    "from apache_beam.options.pipeline_options import PipelineOptions \n",
    "beam_options = PipelineOptions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# CREATING CUSTOM OPTIONS \n",
    "# FIRST NEED TO UNDERSTAND WHAT PYTHON DECORATORS ARE \n",
    "# STATIC VS DYNAMIC METHODS \n",
    "\n",
    "# STATIC METHODS => CAN BE EXECUTED THROUGH THE CLASS INSTEAD OF WHEN THE CLASS HAS BEEN INSTANTIATED OR OBJECT CAN BE CREATED FROM THE CLASS \n",
    "\n",
    "# WITHOUT STATIC METHOD => no attributes so method run directly without the class being instantiated \n",
    "class Formula:\n",
    "    def pow(self, x, y):\n",
    "        return x ** y \n",
    "\n",
    "m = Formula()\n",
    "print(m.pow(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# WITH A STATIC METHOD \n",
    "class Formula: \n",
    "    @staticmethod \n",
    "    def pow(x, y):\n",
    "        return x ** y \n",
    "\n",
    "print(Formula.pow(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ANOTHER EXAMPLE \n",
    "class Greeting: \n",
    "    def display(self, message):\n",
    "        print(message) \n",
    "message = Greeting() \n",
    "print(message.display('HelloWorld')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Greeting: \n",
    "    @staticmethod\n",
    "    def display(message):\n",
    "        print(message) \n",
    "print(Greeting.display('Hello World')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# WORKING WITH CLASS METHODS \n",
    "# CLASS METHODS => STATIC METHODS THAT ACCESS THE INSTANCE ATTRIBUTES & ALSO CAN BE CALLED WITHOUT INSTANTIATING THE CLASS \n",
    "class Greeting: \n",
    "    message = \"Hello\"\n",
    "    @classmethod\n",
    "    def display(mes, finalMessage):\n",
    "        print(mes.message + finalMessage) \n",
    "\n",
    "print(Greeting.display(' World'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING CUSTOM OPTIONS IN ADDITION TO THE STANDARD PIPELINE OPTIONS \n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "    @classmethod \n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument('--input', required=True)\n",
    "        parser.add_argument('--output', required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT HAS DEFAULT DATAFLOW LINK AND HELP TEXT \n",
    "# this allows pipeline to accept input and output as command line arguments \n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "    @classmethod \n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "        '--input',\n",
    "        default= 'gs://dataflow-samples/repairs/repairsfeedback.txt',\n",
    "        help='file path for the input text to process')\n",
    "        parser.add_argument(\n",
    "            '--output', \n",
    "            required=True, \n",
    "            help='The path prefix for the output files'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION => DISRTIBUTED COLLECTION OF DATA \n",
    "# BEAM TRANSFORMS USE PCOLLECTIONS AS INPUTS AND OUTPUTS \n",
    "# IF YOU WANT TO WORK WITH DATA IN YOUR PIPELINE \n",
    "# IT MUST COME FROM A PCOLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING FROM AN EXTERNAL SOURCE \n",
    "# HAVE TO PROVIDE THE LINK \n",
    "import apache_beam as beam \n",
    "\n",
    "lines = pipeline | 'Read the file' >> beam.io.ReadFromText('gs://dataflow-samples/repairs/repairsfeedback.txt') \n",
    "\n",
    "# DO NOT RUN THIS FILE\n",
    "# JUST READ THE CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A PCOLLECTION FROM AN IN MEMORY DATA \n",
    "import apache_beam as beam \n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    lines = (\n",
    "        pipeline \n",
    "        | beam.Create([\n",
    "            'This is true', \n",
    "            'To be, or not to be: that is the question: ',\n",
    "          \"Whether 'tis nobler in the mind to suffer \",\n",
    "          'The slings and arrows of outrageous fortune, ',\n",
    "          'Or to take arms against a sea of troubles, ',\n",
    "        ]))\n",
    "\n",
    "# NOTE: DO NOT RUN THIS AS IT REQUIRES AN OUTPUT TO WORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION FEATURES \n",
    "# PCOLLECTION BE ALWAYS OWNED BY A SPECIFIC PIPELINE \n",
    "# MULTIPLE PIPELINES NO FIT SHARE PCOLLECTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT TYPE \n",
    "# PCOLLECTIONS FIT BE OF ANY TYPE \n",
    "# BUT IF WE DEY DO DISTRIBUTED PROCESSING, BEAM FOR ENCODE THE ELEMENT TYPE AS A BYTE STRING \n",
    "# SO SAY ELEMENTS FIT BE PASSED AROUND TO DISTRIBUTED WORKERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT SCHEMA => JSON, AVRO, DB RECORDS \n",
    "# SCHEMA DEY HELP MAKE WE GET MORE EXPRESSIVE AGGREGATES \n",
    "# WHAT BE AGGREGATES ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTIONS BE IMMUTABLE \n",
    "# YOU NOT FIT CHANGE, ADD OR REMOVE ELEMENTS IF YOU CREATE AM \n",
    "# BEAM TRANSFORM JUST DEY MODIFY EACH ELEMENT DN GENERATE NEW PIPELINE \n",
    "# NO DEY MEAN SAY E GO MODIFY THE ORIGINAL PCOLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION FIT BE ANY SIZE \n",
    "# FIT BE BOUNDED OR UNBOUNDED => EITHER THE SIZE WE KNOW OR WE DONT KNOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEMENT TIME STAMPS \n",
    "# EACH ELEMENT FOR PCOLLECTION GET SOME ASSOCIATED TIME STAMP \n",
    "# THIS BE ASSIGNED BY THE SOURCE WEY CREATE THE PCOLLECTION \n",
    "# SAY IF PIPELINE DEY READ TWEETS, EACH ELEMENT FIT USE THE TIME THE PERSON TWEET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMS \n",
    "# WHAT WE WANT TURN THE DATA INTO \n",
    "# WE GO GO OVER THIS FOR THE EXPERIMENT SECTION \n",
    "# PIPELINE PROCESS DEY LOOK LIKE THIS \n",
    "# DATABASE TABLE -> READ TRANSFORM -> (PCOLLECTION) -> TRANSFORM -> (PCOLLECTION) -> WRITE TRANSFORM -> DATABASE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCOLLECTION IS IMMUTABLE BY DEFINITION BUT \n",
    "# U FIT APPLY DIFFERENT TRANSFORMS FOR THE SAME PCOLLECTION TO CREATE A BRANCING PIPELINE LIKE SO \n",
    "# DATABASE TABLE -> READ DATABASE OF NAMES -> PARDO(EXTRACT STRINGS STARTING WITH A)-> A NAMES \n",
    "# DATABASE TABLE -> READ DATABASE OF NAMES -> PARDO(EXTRACT STRINGS STARTING WITH B)-> B NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CORE BEAM TRANSFORMS \n",
    "# PARDO => PROCESS ELEMENTS IN PARALLEL \n",
    "# GROUP BY KEY => GROUP ELEMENTS BY KEY \n",
    "# CO GROUP BY KEY => GROUP ELEMENTS BY KEY AND JOIN THEM \n",
    "# COMBINE => AGGREGATE ELEMENTS\n",
    "# FLATTEN => EXPAND A COLLECTION OF COLLECTIONS INTO A SINGLE COLLECTION\n",
    "# PARTITION => DIVIDE A COLLECTION INTO A NUMBER OF PARTITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARDO \n",
    "# FILTERING DATASET \n",
    "# TYPE CONVERTING EACH ELEMENT IN THE DATASET \n",
    "# EXTRACTING PARTS OF EACH ELEMENT IN THE DATASET \n",
    "# PERFORMING COMPUTATIONS ON EACH ELEMENT IN THE DATASET \n",
    "\n",
    "words = ... # input pcollection of strings \n",
    "\n",
    "# do function to perform on each element in the input pcollection\n",
    "class ComputeLength(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [len(element)] \n",
    "\n",
    "# apply the do function to the input pcollection \n",
    "word_length = words | beam.ParDo(ComputeLength()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT BE DO FUNCTION \n",
    "# THEN DEY DEFINE THE PIPLELINE EXTRACT PROCESSING TASKS \n",
    "# THE CODE FOR FULFIL THESE TWO REQUIREMENTS \n",
    "# MAKE SURE WE DEY FULFUL THIS BEFORE WE USE THE DO FUNCTIONS \n",
    "# FUNCTION OBJECT FOR BE SERLIALISABLE \n",
    "# FUNCTION OBJECT FOR THE THREAD-COMPATIBLE AND FOR KNOW SAY BEAM SDKs NO BE THREAD SAFE \n",
    "# ALSO MAKE THE FUNCTIONS IDEMPOTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THE FUNCTIONS BE STRAIGHFOWARD \n",
    "# WE FIT PROVIDE LIGHT WEIGHT DO FUNCTIONS ONE LINERS \n",
    "word_length = words | beam.ParDo(lambda x: [len(x)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO FUNCTION LIFECYCLE \n",
    "# DO DEEP INTO SERLLIALISATION AND DESERIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY KEY AND UNBOUNDED PCOLLECTIONS \n",
    "\n",
    "\n",
    "# U FOR USE NON-GLOBAL KEY WINDOWING OR AGGREGATION TRIGGER TO PERFORM GROUPBYKEY OR COGROUPBYKEY \n",
    "# UNBOUNDED COLLECTIONS \n",
    "# WINDOWING OR TRIGGERS FOR ALLOW GROUPING TO OPERATE ON LOGICAL FINITE BUNDLES OF DATA WITHIN THE \n",
    "# UNBOUNDED DATA STREAMS \n",
    "\n",
    "# IF WE NO USE AM E GO THROW THIS ERROR =>  IllegalStateException error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COGROUPBY KEY => JOIN KEY VALUE PAIRS WEY BE THE SAME TYPE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--runner RUNNER] [--streaming]\n",
      "                             [--resource_hint RESOURCE_HINTS]\n",
      "                             [--beam_services BEAM_SERVICES]\n",
      "                             [--type_check_strictness {ALL_REQUIRED,DEFAULT_TO_ANY}]\n",
      "                             [--type_check_additional TYPE_CHECK_ADDITIONAL]\n",
      "                             [--no_pipeline_type_check] [--runtime_type_check]\n",
      "                             [--performance_runtime_type_check]\n",
      "                             [--allow_non_deterministic_key_coders]\n",
      "                             [--allow_unsafe_triggers]\n",
      "                             [--no_direct_runner_use_stacked_bundle]\n",
      "                             [--direct_runner_bundle_repeat DIRECT_RUNNER_BUNDLE_REPEAT]\n",
      "                             [--direct_num_workers DIRECT_NUM_WORKERS]\n",
      "                             [--direct_running_mode {in_memory,multi_threading,multi_processing}]\n",
      "                             [--direct_embed_docker_python]\n",
      "                             [--dataflow_endpoint DATAFLOW_ENDPOINT]\n",
      "                             [--project PROJECT] [--job_name JOB_NAME]\n",
      "                             [--staging_location STAGING_LOCATION]\n",
      "                             [--temp_location TEMP_LOCATION] [--region REGION]\n",
      "                             [--service_account_email SERVICE_ACCOUNT_EMAIL]\n",
      "                             [--no_auth]\n",
      "                             [--template_location TEMPLATE_LOCATION]\n",
      "                             [--label LABELS] [--update]\n",
      "                             [--transform_name_mapping TRANSFORM_NAME_MAPPING]\n",
      "                             [--enable_streaming_engine]\n",
      "                             [--dataflow_kms_key DATAFLOW_KMS_KEY]\n",
      "                             [--create_from_snapshot CREATE_FROM_SNAPSHOT]\n",
      "                             [--flexrs_goal {COST_OPTIMIZED,SPEED_OPTIMIZED}]\n",
      "                             [--dataflow_service_option DATAFLOW_SERVICE_OPTIONS]\n",
      "                             [--enable_hot_key_logging]\n",
      "                             [--enable_artifact_caching]\n",
      "                             [--impersonate_service_account IMPERSONATE_SERVICE_ACCOUNT]\n",
      "                             [--gcp_oauth_scope GCP_OAUTH_SCOPES]\n",
      "                             [--hdfs_host HDFS_HOST] [--hdfs_port HDFS_PORT]\n",
      "                             [--hdfs_user HDFS_USER] [--hdfs_full_urls]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--max_num_workers MAX_NUM_WORKERS]\n",
      "                             [--autoscaling_algorithm {NONE,THROUGHPUT_BASED}]\n",
      "                             [--worker_machine_type MACHINE_TYPE]\n",
      "                             [--disk_size_gb DISK_SIZE_GB]\n",
      "                             [--worker_disk_type DISK_TYPE]\n",
      "                             [--worker_region WORKER_REGION]\n",
      "                             [--worker_zone WORKER_ZONE] [--zone ZONE]\n",
      "                             [--network NETWORK] [--subnetwork SUBNETWORK]\n",
      "                             [--worker_harness_container_image WORKER_HARNESS_CONTAINER_IMAGE]\n",
      "                             [--sdk_container_image SDK_CONTAINER_IMAGE]\n",
      "                             [--sdk_harness_container_image_overrides SDK_HARNESS_CONTAINER_IMAGE_OVERRIDES]\n",
      "                             [--default_sdk_harness_log_level DEFAULT_SDK_HARNESS_LOG_LEVEL]\n",
      "                             [--sdk_harness_log_level_overrides SDK_HARNESS_LOG_LEVEL_OVERRIDES]\n",
      "                             [--use_public_ips] [--no_use_public_ips]\n",
      "                             [--min_cpu_platform MIN_CPU_PLATFORM]\n",
      "                             [--dataflow_worker_jar DATAFLOW_WORKER_JAR]\n",
      "                             [--dataflow_job_file DATAFLOW_JOB_FILE]\n",
      "                             [--experiment EXPERIMENTS]\n",
      "                             [--number_of_worker_harness_threads NUMBER_OF_WORKER_HARNESS_THREADS]\n",
      "                             [--profile_cpu] [--profile_memory]\n",
      "                             [--profile_location PROFILE_LOCATION]\n",
      "                             [--profile_sample_rate PROFILE_SAMPLE_RATE]\n",
      "                             [--requirements_file REQUIREMENTS_FILE]\n",
      "                             [--requirements_cache REQUIREMENTS_CACHE]\n",
      "                             [--requirements_cache_only_sources]\n",
      "                             [--setup_file SETUP_FILE]\n",
      "                             [--beam_plugin BEAM_PLUGINS]\n",
      "                             [--pickle_library {cloudpickle,default,dill}]\n",
      "                             [--save_main_session]\n",
      "                             [--sdk_location SDK_LOCATION]\n",
      "                             [--extra_package EXTRA_PACKAGES]\n",
      "                             [--prebuild_sdk_container_engine PREBUILD_SDK_CONTAINER_ENGINE]\n",
      "                             [--prebuild_sdk_container_base_image PREBUILD_SDK_CONTAINER_BASE_IMAGE]\n",
      "                             [--cloud_build_machine_type CLOUD_BUILD_MACHINE_TYPE]\n",
      "                             [--docker_registry_push_url DOCKER_REGISTRY_PUSH_URL]\n",
      "                             [--job_endpoint JOB_ENDPOINT]\n",
      "                             [--artifact_endpoint ARTIFACT_ENDPOINT]\n",
      "                             [--job_server_timeout JOB_SERVER_TIMEOUT]\n",
      "                             [--environment_type ENVIRONMENT_TYPE]\n",
      "                             [--environment_config ENVIRONMENT_CONFIG]\n",
      "                             [--environment_option ENVIRONMENT_OPTIONS]\n",
      "                             [--sdk_worker_parallelism SDK_WORKER_PARALLELISM]\n",
      "                             [--environment_cache_millis ENVIRONMENT_CACHE_MILLIS]\n",
      "                             [--output_executable_path OUTPUT_EXECUTABLE_PATH]\n",
      "                             [--artifacts_dir ARTIFACTS_DIR]\n",
      "                             [--job_port JOB_PORT]\n",
      "                             [--artifact_port ARTIFACT_PORT]\n",
      "                             [--expansion_port EXPANSION_PORT]\n",
      "                             [--job_server_java_launcher JOB_SERVER_JAVA_LAUNCHER]\n",
      "                             [--job_server_jvm_properties JOB_SERVER_JVM_PROPERTIES]\n",
      "                             [--flink_master FLINK_MASTER]\n",
      "                             [--flink_version {1.12,1.13,1.14,1.15}]\n",
      "                             [--flink_job_server_jar FLINK_JOB_SERVER_JAR]\n",
      "                             [--flink_submit_uber_jar]\n",
      "                             [--parallelism PARALLELISM]\n",
      "                             [--max_parallelism MAX_PARALLELISM]\n",
      "                             [--spark_master_url SPARK_MASTER_URL]\n",
      "                             [--spark_job_server_jar SPARK_JOB_SERVER_JAR]\n",
      "                             [--spark_submit_uber_jar]\n",
      "                             [--spark_rest_url SPARK_REST_URL]\n",
      "                             [--spark_version {2,3}]\n",
      "                             [--on_success_matcher ON_SUCCESS_MATCHER]\n",
      "                             [--dry_run DRY_RUN]\n",
      "                             [--wait_until_finish_duration WAIT_UNTIL_FINISH_DURATION]\n",
      "                             [--pubsub_root_url PUBSUBROOTURL]\n",
      "                             [--s3_access_key_id S3_ACCESS_KEY_ID]\n",
      "                             [--s3_secret_access_key S3_SECRET_ACCESS_KEY]\n",
      "                             [--s3_session_token S3_SESSION_TOKEN]\n",
      "                             [--s3_endpoint_url S3_ENDPOINT_URL]\n",
      "                             [--s3_region_name S3_REGION_NAME]\n",
      "                             [--s3_api_version S3_API_VERSION]\n",
      "                             [--s3_verify S3_VERIFY] [--s3_disable_ssl]\n",
      "                             [--input INPUT] --output OUTPUT\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessmensa/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:3386: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# COMBINE => COMBINING ELEMENTS OR VALUES IN PCOLLECTIOONS \n",
    "pc = [1, 10, 100, 1000] \n",
    "\n",
    "def boundedsum(values, bounds=500):\n",
    "    return min(sum(values), bounds) \n",
    "\n",
    "small_sum = pc | beam.CombineGlobally(boundedsum)\n",
    "large_sum = pc | beam.CombineGlobally(boundedsum, bounds=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED COMBINATIONS USING COMBINEFN \n",
    "# NB: NEED TO UNDERSTAND COMBINERS WELL \n",
    "# NOTE: ALL COMBINERS SHOULD HAVE A MORE SOPHISTICATED ACCUMULATOR \n",
    "\n",
    "\n",
    "# FOR MORE COMPLEX FUNCTIONS, DEFINE A SUBCLASS OF COMBINEFN \n",
    "# YOU SHOULD USE A COMBINE FN IF FUNCTION REQUIRES A MORE SOPHISTICATED ACCUMULATOR \n",
    "\n",
    "# GENERAL COMBINING OPERATION CONSISTS OF FOUR STEPS \n",
    "# 1. CREATE AN ACCUMULATOR 2. ADD INPUT 3. MERGE ACCUMULATORS 4. EXTRACT OUTPUT \n",
    "\n",
    "pc = ... \n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0) \n",
    "\n",
    "    def add_input(self, sum_count, input):\n",
    "        (sum, count) = sum_count \n",
    "        return sum + input, count + 1 \n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators) \n",
    "        return sum(sums), sum(counts) \n",
    "\n",
    "    def extract_output(self, sum_count):\n",
    "        (sum, count) = sum_count \n",
    "        return sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINING ALL PCOLLCTIONS INTO A SINGLE VALUE \n",
    "\n",
    "pc = ... \n",
    "\n",
    "average = pc | beam.CombineGlobally(AverageFn()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE AND GLOBAL WINDOWING ?? COME BACK TO THIS PART \n",
    "# what is global windowing in the first place ?? \n",
    "# => If your input PCollection uses the default global windowing, the default behavior is to return a\n",
    "#  PCollection containing one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE VALUES in a keyed PCOLLECTION ?? COME BACK TO THIS PART TOO  \n",
    "player_accuracies = ...\n",
    "\n",
    "averageaccuracyperplayer = (\n",
    "    player_accuracies \n",
    "    | beam.CombinePerKey(beam.combiners.MeanCombineFn())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTEN => MERGE MULTIPLE PCOLLECTIONS INTO A SINGLE PCOLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
